import os
import gradio as gr
import numpy as np
from enum import Enum
import db_examples
import cv2


from demo_utils1 import *

from misc_utils.train_utils import unit_test_create_model
from misc_utils.image_utils import save_tensor_to_gif, save_tensor_to_images
import os
from PIL import Image
import torch
import torchvision
from torchvision import transforms
from einops import rearrange
import imageio
import time

from torchvision.transforms import functional as F
from torch.hub import download_url_to_file

import os
import spaces

# æ¨ç†è®¾ç½®
from pl_trainer.inference.inference import InferenceIP2PVideo
from tqdm import tqdm


# if not os.path.exists(filename):
#     original_path = os.getcwd()
#     base_path = './models'
#     os.makedirs(base_path, exist_ok=True)

#     # ç›´æ¥åœ¨ä»£ç ä¸­å†™å…¥ Tokenï¼ˆæ³¨æ„å®‰å…¨é£é™©ï¼‰
#     GIT_TOKEN = "955b8ea91095840b76fe38b90a088c200d4c813c"
#     repo_url = f"https://YeFang:{GIT_TOKEN}@code.openxlab.org.cn/YeFang/RIV_models.git"

#     try:
#         if os.system(f'git clone {repo_url} {base_path}') != 0:
#             raise RuntimeError("Git å…‹éš†å¤±è´¥")
#         os.chdir(base_path)
#         if os.system('git lfs pull') != 0:
#             raise RuntimeError("Git LFS æ‹‰å–å¤±è´¥")
#     finally:
#         os.chdir(original_path)

def tensor_to_pil_image(x):
    """
    å°† 4D PyTorch å¼ é‡è½¬æ¢ä¸º PIL å›¾åƒã€‚
    """
    x = x.float()  # ç¡®ä¿å¼ é‡ç±»å‹ä¸º float
    grid_img = torchvision.utils.make_grid(x, nrow=4).permute(1, 2, 0).detach().cpu().numpy()
    grid_img = (grid_img * 255).clip(0, 255).astype("uint8")  # å°† [0, 1] èŒƒå›´è½¬æ¢ä¸º [0, 255]
    return Image.fromarray(grid_img)

def frame_to_batch(x):
    """
    å°†å¸§ç»´åº¦è½¬æ¢ä¸ºæ‰¹æ¬¡ç»´åº¦ã€‚
    """
    return rearrange(x, 'b f c h w -> (b f) c h w')

def clip_image(x, min=0., max=1.):
    """
    å°†å›¾åƒå¼ é‡è£å‰ªåˆ°æŒ‡å®šçš„æœ€å°å’Œæœ€å¤§å€¼ã€‚
    """
    return torch.clamp(x, min=min, max=max)

def unnormalize(x):
    """
    å°†å¼ é‡èŒƒå›´ä» [-1, 1] è½¬æ¢åˆ° [0, 1]ã€‚
    """
    return (x + 1) / 2


# è¯»å–å›¾åƒæ–‡ä»¶
def read_images_from_directory(directory, num_frames=16):
    images = []
    for i in range(num_frames):
        img_path = os.path.join(directory, f'{i:04d}.png')
        img = imageio.imread(img_path)
        images.append(torch.tensor(img).permute(2, 0, 1))  # Convert to Tensor (C, H, W)
    return images

def load_and_process_images(folder_path):
    """
    è¯»å–æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾ç‰‡ï¼Œå°†å®ƒä»¬è½¬æ¢ä¸º [-1, 1] èŒƒå›´çš„å¼ é‡å¹¶è¿”å›ä¸€ä¸ª 4D å¼ é‡ã€‚
    """
    processed_images = []
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Lambda(lambda x: x * 2 - 1)  # å°† [0, 1] è½¬æ¢ä¸º [-1, 1]
    ])
    for filename in sorted(os.listdir(folder_path)):
        if filename.endswith(".png"):
            img_path = os.path.join(folder_path, filename)
            image = Image.open(img_path).convert("RGB")
            processed_image = transform(image)
            processed_images.append(processed_image)
    return torch.stack(processed_images)  # è¿”å› 4D å¼ é‡

def load_and_process_video(video_path, num_frames=16, crop_size=512):
    """
    è¯»å–è§†é¢‘æ–‡ä»¶ä¸­çš„å‰ num_frames å¸§ï¼Œå°†æ¯ä¸€å¸§è½¬æ¢ä¸º [-1, 1] èŒƒå›´çš„å¼ é‡ï¼Œ
    å¹¶è¿›è¡Œä¸­å¿ƒè£å‰ªè‡³ crop_size x crop_sizeï¼Œè¿”å›ä¸€ä¸ª 4D å¼ é‡ã€‚
    """
    processed_frames = []
    transform = transforms.Compose([
        transforms.CenterCrop(crop_size),       # ä¸­å¿ƒè£å‰ª
        transforms.ToTensor(),
        transforms.Lambda(lambda x: x * 2 - 1)  # å°† [0, 1] è½¬æ¢ä¸º [-1, 1]
    ])
    
    # ä½¿ç”¨ OpenCV è¯»å–è§†é¢‘
    cap = cv2.VideoCapture(video_path)
    
    if not cap.isOpened():
        raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")

    frame_count = 0

    while frame_count < num_frames:
        ret, frame = cap.read()
        if not ret:
            break  # è§†é¢‘å¸§è¯»å–å®Œæ¯•æˆ–è§†é¢‘å¸§ä¸è¶³
        
        # è½¬æ¢ä¸º RGB æ ¼å¼
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        image = Image.fromarray(frame)
        
        # åº”ç”¨è½¬æ¢
        processed_frame = transform(image)
        processed_frames.append(processed_frame)
        
        frame_count += 1

    cap.release()  # é‡Šæ”¾è§†é¢‘èµ„æº

    if len(processed_frames) < num_frames:
        raise ValueError(f"è§†é¢‘å¸§ä¸è¶³ {num_frames} å¸§ï¼Œä»…æ‰¾åˆ° {len(processed_frames)} å¸§ã€‚")

    return torch.stack(processed_frames)  # è¿”å› 4D å¼ é‡ (å¸§æ•°, é€šé“æ•°, é«˜åº¦, å®½åº¦)


def clear_cache(output_path):
    if os.path.exists(output_path):
        os.remove(output_path)
    return None


#! åŠ è½½æ¨¡å‹
# é…ç½®è·¯å¾„å’ŒåŠ è½½æ¨¡å‹
config_path = 'configs/instruct_v2v_ic_gradio.yaml'
diffusion_model = unit_test_create_model(config_path)
diffusion_model = diffusion_model.to('cuda')

# åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
# ckpt_path = 'models/relvid_mm_sd15_fbc_unet.pth' #! change
# ckpt_path = 'tmp/pytorch_model.bin'
# ä¸‹è½½æ–‡ä»¶

os.makedirs('models', exist_ok=True)
model_path = "models/relvid_mm_sd15_fbc_unet.pth"

if not os.path.exists(model_path):
    download_url_to_file(url='https://huggingface.co/aleafy/RelightVid/resolve/main/relvid_mm_sd15_fbc_unet.pth', dst=model_path)


ckpt = torch.load(model_path, map_location='cpu')
diffusion_model.load_state_dict(ckpt, strict=False)


# import pdb; pdb.set_trace()

# æ›´æ”¹å…¨å±€ä¸´æ—¶ç›®å½•
new_tmp_dir = "./demo/gradio_bg"
os.makedirs(new_tmp_dir, exist_ok=True)

# import pdb; pdb.set_trace()

def save_video_from_frames(image_pred, save_pth, fps=8):
    """
    å°† image_pred ä¸­çš„å¸§ä¿å­˜ä¸ºè§†é¢‘æ–‡ä»¶ã€‚

    å‚æ•°:
    - image_pred: Tensorï¼Œå½¢çŠ¶ä¸º (1, 16, 3, 512, 512)
    - save_pth: ä¿å­˜è§†é¢‘çš„è·¯å¾„ï¼Œä¾‹å¦‚ "output_video.mp4"
    - fps: è§†é¢‘çš„å¸§ç‡
    """
    # è§†é¢‘å‚æ•°
    num_frames = image_pred.shape[1]
    frame_height, frame_width = 512, 512  # ç›®æ ‡å°ºå¯¸
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # ä½¿ç”¨ mp4 ç¼–ç æ ¼å¼

    # åˆ›å»º VideoWriter å¯¹è±¡
    out = cv2.VideoWriter(save_pth, fourcc, fps, (frame_width, frame_height))

    for i in range(num_frames):
        # åå½’ä¸€åŒ– + è½¬æ¢ä¸º 0-255 èŒƒå›´
        pred_frame = clip_image(unnormalize(image_pred[0][i].unsqueeze(0))) * 255
        pred_frame_resized = pred_frame.squeeze(0).detach().cpu()  # (3, 512, 512)
        pred_frame_resized = pred_frame_resized.permute(1, 2, 0).numpy().astype("uint8")  # (512, 512, 3)

        # Resize åˆ° 256x256
        pred_frame_resized = cv2.resize(pred_frame_resized, (frame_width, frame_height))

        # å°† RGB è½¬ä¸º BGRï¼ˆå› ä¸º OpenCV ä½¿ç”¨ BGR æ ¼å¼ï¼‰
        pred_frame_bgr = cv2.cvtColor(pred_frame_resized, cv2.COLOR_RGB2BGR)

        # å†™å…¥å¸§åˆ°è§†é¢‘
        out.write(pred_frame_bgr)

    # é‡Šæ”¾ VideoWriter èµ„æº
    out.release()
    print(f"è§†é¢‘å·²ä¿å­˜è‡³ {save_pth}")


inf_pipe = InferenceIP2PVideo(
        diffusion_model.unet, 
        scheduler='ddpm',
        num_ddim_steps=20
    )


def process_example(*args):
    v_index = args[0]
    select_e = db_examples.background_conditioned_examples[int(v_index)-1]
    input_fg_path = select_e[1]
    input_bg_path = select_e[2]
    result_video_path = select_e[-1]
    # input_fg_img = args[1]  # ç¬¬ 0 ä¸ªå‚æ•°
    # input_bg_img = args[2]  # ç¬¬ 1 ä¸ªå‚æ•°
    # result_video_img = args[-1]  # æœ€åä¸€ä¸ªå‚æ•°
    
    input_fg = input_fg_path.replace("frames/0000.png", "cropped_video.mp4")
    input_bg = input_bg_path.replace("frames/0000.png", "cropped_video.mp4")
    result_video = result_video_path.replace(".png", ".mp4")
    
    return input_fg, input_bg, result_video



# ä¼ªå‡½æ•°å ä½ï¼ˆç”Ÿæˆç©ºç™½è§†é¢‘ï¼‰
@spaces.GPU
def dummy_process(input_fg, input_bg, prompt):
    # import pdb; pdb.set_trace()

    diffusion_model.to(torch.float16)
    fg_tensor = load_and_process_video(input_fg).cuda().unsqueeze(0).to(dtype=torch.float16)
    bg_tensor = load_and_process_video(input_bg).cuda().unsqueeze(0).to(dtype=torch.float16) # (1, 16, 4, 64, 64)

    cond_fg_tensor = diffusion_model.encode_image_to_latent(fg_tensor)  # (1, 16, 4, 64, 64)
    cond_bg_tensor = diffusion_model.encode_image_to_latent(bg_tensor)
    cond_tensor = torch.cat((cond_fg_tensor, cond_bg_tensor), dim=2)

    # åˆå§‹åŒ–æ½œå˜é‡
    init_latent = torch.randn_like(cond_fg_tensor)    

    # EDIT_PROMPT = 'change the background'
    EDIT_PROMPT = prompt
    VIDEO_CFG = 1.2
    TEXT_CFG = 7.5
    text_cond = diffusion_model.encode_text([EDIT_PROMPT])  # (1, 77, 768)
    text_uncond = diffusion_model.encode_text([''])
    # to float16
    print('------------to float 16----------------')
    init_latent, text_cond, text_uncond, cond_tensor = (
        init_latent.to(dtype=torch.float16),
        text_cond.to(dtype=torch.float16),
        text_uncond.to(dtype=torch.float16),
        cond_tensor.to(dtype=torch.float16)
    )
    inf_pipe.unet.to(torch.float16)
    latent_pred = inf_pipe(
        latent=init_latent,
        text_cond=text_cond,
        text_uncond=text_uncond,
        img_cond=cond_tensor,
        text_cfg=TEXT_CFG,
        img_cfg=VIDEO_CFG,
    )['latent']
    

    image_pred = diffusion_model.decode_latent_to_image(latent_pred) # (1,16,3,512,512)
    output_path = os.path.join(new_tmp_dir, f"output_{int(time.time())}.mp4")
    # clear_cache(output_path)
    
    save_video_from_frames(image_pred, output_path)
    # import pdb; pdb.set_trace()
    # fps = 8
    # frames = []
    # for i in range(16):
    #     pred_frame = clip_image(unnormalize(image_pred[0][i].unsqueeze(0))) * 255
    #     pred_frame_resized = pred_frame.squeeze(0).detach().cpu() #(3,512,512)
    #     pred_frame_resized = pred_frame_resized.permute(1, 2, 0).detach().cpu().numpy().astype("uint8") #(512,512,3) np
    #     Image.fromarray(pred_frame_resized).save(save_pth)

    # # ç”Ÿæˆä¸€ä¸ªç®€å•çš„é»‘è‰²è§†é¢‘ä½œä¸ºç¤ºä¾‹
    # output_path = os.path.join(new_tmp_dir, "output.mp4")
    # fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    # out = cv2.VideoWriter(output_path, fourcc, 20.0, (512, 512))

    # for _ in range(60):  # ç”Ÿæˆ 3 ç§’çš„è§†é¢‘ï¼ˆ20fpsï¼‰
    #     frame = np.zeros((512, 512, 3), dtype=np.uint8)
    #     out.write(frame)
    # out.release()
    torch.cuda.empty_cache()

    return output_path

# æšä¸¾ç±»ç”¨äºèƒŒæ™¯é€‰æ‹©
class BGSource(Enum):
    UPLOAD = "Use Background Video"
    UPLOAD_FLIP = "Use Flipped Background Video"
    UPLOAD_REVERSE = "Use Reversed Background Video"


# Quick prompts ç¤ºä¾‹
# quick_prompts = [
#     'beautiful woman, fantasy setting',
#     'beautiful woman, neon dynamic lighting',
#     'man in suit, tunel lighting',
#     'animated mouse, aesthetic lighting',
#     'robot warrior, a sunset background',
#     'yellow cat, reflective wet beach',
#     'camera, dock, calm sunset',
#     'astronaut, dim lighting',
#     'astronaut, colorful balloons',
#     'astronaut, desert landscape'
# ]

# quick_prompts = [
#     'beautiful woman',
#     'handsome man',
#     'beautiful woman, cinematic lighting',
#     'handsome man, cinematic lighting',
#     'beautiful woman, natural lighting',
#     'handsome man, natural lighting',
#     'beautiful woman, neo punk lighting, cyberpunk',
#     'handsome man, neo punk lighting, cyberpunk',
# ]


quick_prompts = [
    'beautiful woman',
    'handsome man',
    'beautiful woman, cinematic lighting',
    'handsome man, cinematic lighting',
    'beautiful woman, natural lighting',
    'handsome man, natural lighting',
    'beautiful woman, warm lighting',
    'handsome man, soft lighting',
    'change the background lighting',
]


quick_prompts = [[x] for x in quick_prompts]

# css = """
# #foreground-gallery {
#     width: 700 !important; /* é™åˆ¶æœ€å¤§å®½åº¦ */
#     max-width: 700px !important; /* é¿å…å®ƒè‡ªåŠ¨å˜å®½ */
#     flex: none !important; /* è®©å®ƒä¸è‡ªåŠ¨æ‰©å±• */
# }
# """

# Gradio UI ç»“æ„
block = gr.Blocks().queue()
with block:
    with gr.Row():
        # gr.Markdown("## RelightVid (Relighting with Foreground and Background Video Condition)")
        gr.Markdown("# ğŸ’¡RelightVid  \n### Relighting with Foreground and Background Video Condition")

    with gr.Row():
        with gr.Column():
            with gr.Row():
                input_fg = gr.Video(label="Foreground Video", height=380, width=420, visible=True)
                input_bg = gr.Video(label="Background Video", height=380, width=420, visible=True)
            
            segment_button = gr.Button(value="Video Segmentation")
            with gr.Accordion("Segmentation Options", open=False):
                # å¦‚æœç”¨æˆ·ä¸ä½¿ç”¨ point_promptï¼Œè€Œæ˜¯ç›´æ¥æä¾›åæ ‡ï¼Œåˆ™ä½¿ç”¨ x, y
                with gr.Row():
                    x_coord = gr.Slider(label="X Coordinate (Point Prompt Ratio)", minimum=0.0, maximum=1.0, value=0.5, step=0.01)
                    y_coord = gr.Slider(label="Y Coordinate (Point Prompt Ratio)", minimum=0.0, maximum=1.0, value=0.5, step=0.01)
                        

            fg_gallery = gr.Gallery(height=150, object_fit='contain', label='Foreground Quick List', value=db_examples.fg_samples, columns=5, allow_preview=False)
            bg_gallery = gr.Gallery(height=450, object_fit='contain', label='Background Quick List', value=db_examples.bg_samples, columns=5, allow_preview=False)
            

            with gr.Group():
            #     with gr.Row():
            #         num_samples = gr.Slider(label="Videos", minimum=1, maximum=12, value=1, step=1)
            #         seed = gr.Number(label="Seed", value=12345, precision=0)
                with gr.Row():
                    video_width = gr.Slider(label="Video Width", minimum=256, maximum=1024, value=512, step=64, visible=False)
                    video_height = gr.Slider(label="Video Height", minimum=256, maximum=1024, value=512, step=64, visible=False)

            # with gr.Accordion("Advanced options", open=False):
            #     steps = gr.Slider(label="Steps", minimum=1, maximum=100, value=20, step=1)
            #     cfg = gr.Slider(label="CFG Scale", minimum=1.0, maximum=32.0, value=7.0, step=0.01)
            #     highres_scale = gr.Slider(label="Highres Scale", minimum=1.0, maximum=3.0, value=1.5, step=0.01)
            #     highres_denoise = gr.Slider(label="Highres Denoise", minimum=0.1, maximum=0.9, value=0.5, step=0.01)
            #     a_prompt = gr.Textbox(label="Added Prompt", value='best quality')
            #     n_prompt = gr.Textbox(label="Negative Prompt", value='lowres, bad anatomy, bad hands, cropped, worst quality')
            #     normal_button = gr.Button(value="Compute Normal (4x Slower)")

        with gr.Column():
            result_video = gr.Video(label='Output Video', height=700, width=700, visible=True)

            prompt = gr.Textbox(label="Prompt")
            bg_source = gr.Radio(choices=[e.value for e in BGSource],
                                value=BGSource.UPLOAD.value,
                                label="Background Source", type='value')

            example_prompts = gr.Dataset(samples=quick_prompts, label='Prompt Quick List', components=[prompt])
            relight_button = gr.Button(value="Relight")
            # fg_gallery = gr.Gallery(witdth=400, object_fit='contain', label='Foreground Quick List', value=db_examples.bg_samples, columns=4, allow_preview=False)
            # fg_gallery = gr.Gallery(
            #     height=380, 
            #     object_fit='contain', 
            #     label='Foreground Quick List', 
            #     value=db_examples.fg_samples, 
            #     columns=4, 
            #     allow_preview=False,
            #     elem_id="foreground-gallery"  # ğŸ‘ˆ æ·»åŠ  elem_id
            # )


    # è¾“å…¥åˆ—è¡¨
    # ips = [input_fg, input_bg, prompt, video_width, video_height, num_samples, seed, steps, a_prompt, n_prompt, cfg, highres_scale, highres_denoise, bg_source]
    ips = [input_fg, input_bg, prompt]

    # æŒ‰é’®ç»‘å®šå¤„ç†å‡½æ•°
    # relight_button.click(fn=lambda: None, inputs=[], outputs=[result_video])

    relight_button.click(fn=dummy_process, inputs=ips, outputs=[result_video])
    
    # normal_button.click(fn=dummy_process, inputs=ips, outputs=[result_video])

    # èƒŒæ™¯åº“é€‰æ‹©
    def bg_gallery_selected(gal, evt: gr.SelectData):
        # import pdb; pdb.set_trace()
        # img_path = gal[evt.index][0]
        img_path = db_examples.bg_samples[evt.index]
        video_path = img_path.replace('frames/0000.png', 'cropped_video.mp4')
        return video_path

    bg_gallery.select(bg_gallery_selected, inputs=bg_gallery, outputs=input_bg)

    def fg_gallery_selected(gal, evt: gr.SelectData):
        # import pdb; pdb.set_trace()
        # img_path = gal[evt.index][0]
        img_path = db_examples.fg_samples[evt.index]
        video_path = img_path.replace('frames/0000.png', 'cropped_video.mp4')
        return video_path

    fg_gallery.select(fg_gallery_selected, inputs=fg_gallery, outputs=input_fg)

    input_fg_img = gr.Image(label="Foreground Video", visible=False)
    input_bg_img = gr.Image(label="Background Video", visible=False)
    result_video_img = gr.Image(label="Output Video", visible=False)

    v_index = gr.Textbox(label="ID", visible=False)
    example_prompts.click(lambda x: x[0], inputs=example_prompts, outputs=prompt, show_progress=False, queue=False)

    # ç¤ºä¾‹
    # dummy_video_for_outputs = gr.Video(visible=False, label='Result')
    gr.Examples(
        # fn=lambda *args: args[-1],
        fn=process_example,
        examples=db_examples.background_conditioned_examples,
        # inputs=[v_index, input_fg_img, input_bg_img, prompt, bg_source, video_width, video_height, result_video_img],
        inputs=[v_index, input_fg_img, input_bg_img, prompt, bg_source, result_video_img],
        outputs=[input_fg, input_bg, result_video],
        run_on_click=True, examples_per_page=1024
    )

# å¯åŠ¨ Gradio åº”ç”¨
# block.launch(server_name='0.0.0.0', server_port=10002, share=True)
block.launch()
